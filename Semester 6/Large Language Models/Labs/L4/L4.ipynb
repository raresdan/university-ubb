{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b90168a-5599-4c81-b725-5ad63ba3f0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install deepeval\n",
    "#!pip install rouge-score\n",
    "#!pip install evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df3f64d-b96f-4133-9b64-6715d9328998",
   "metadata": {},
   "source": [
    "# Classical NLP testing and LLM-judge testing with Deepeval\n",
    "\n",
    "## https://github.com/confident-ai/deepeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26652e0c-7d09-4631-91d8-ce73440a1538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 F1: 1.0\n",
      "ROUGE-L F1: 0.8333333333333334\n",
      "BLEU score: 0.5623413251903491\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "import evaluate\n",
    "\n",
    "# expected = \"ana are mere, tu ce ai?\"\n",
    "# predicted = \"are ana mere\"\n",
    "\n",
    "predicted = \"ana are mere  tu ce ai\"\n",
    "expected = \"are ana mere tu ce ai\"\n",
    "\n",
    "# ROUGE\n",
    "rouge = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "rouge_scores = rouge.score(expected, predicted)\n",
    "\n",
    "print(\"ROUGE-1 F1:\", rouge_scores[\"rouge1\"].fmeasure)\n",
    "print(\"ROUGE-L F1:\", rouge_scores[\"rougeL\"].fmeasure)\n",
    "\n",
    "# BLEU\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "bleu_result = bleu.compute(predictions=[predicted], references=[[expected]])\n",
    "\n",
    "print(\"BLEU score:\", bleu_result[\"bleu\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d32d6b0-752a-4cbe-9614-9d67f819bb7b",
   "metadata": {},
   "source": [
    "### We will set Deepeval to use as LLM-judge a locally hosted LLM\n",
    "\n",
    "#### Option 1: with LMStudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d880833b-03d4-47b6-af79-ddfcc9e6e512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ™Œ Congratulations! You're now using a local model for all evals that require an\n",
      "LLM.\n"
     ]
    }
   ],
   "source": [
    "### YOU NEED TO HAVE LMSTUDIO STARTED AND ONE MODEL LOADED\n",
    "### REPLACE THE MODEL WITH THE ONE THAT YOU LOADED\n",
    "\n",
    "!deepeval set-local-model --model-name=\"phi-4@q8_0\" --base-url=\"http://localhost:1234/v1/\" --api-key=\"test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaec9a3b-c83b-4031-b48f-33d553d5dd84",
   "metadata": {},
   "source": [
    "#### Option 2, create a custom LLM handler (through transformers):\n",
    "\n",
    "#### https://docs.confident-ai.com/guides/guides-using-custom-llms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02b1240d-72ef-4278-b564-0758838b6df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ|100% (1/1) [Time Taken: 00:06,  6.98s/test case]\n"
     ]
    }
   ],
   "source": [
    "from deepeval import assert_test\n",
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "\n",
    "def test_case():\n",
    "    correctness_metric = GEval(\n",
    "        name=\"Correctness\",\n",
    "        criteria=\"Determine if the 'actual output' is correct based on the 'expected output'.\",\n",
    "        evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],\n",
    "        threshold=0.5\n",
    "    )\n",
    "    test_case = LLMTestCase(\n",
    "        input=\"What if these shoes don't fit?\",\n",
    "        actual_output=\"You have 30 days to get a full refund at no extra cost.\",\n",
    "        expected_output=\"We offer a 30-day full refund at no extra costs.\",\n",
    "        retrieval_context=[\"All customers are eligible for a 30 day full refund at no extra costs.\"]\n",
    "    )\n",
    "    assert_test(test_case, [correctness_metric])\n",
    "\n",
    "test_case()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbe2799-1dae-470a-81df-daaeab8b21d6",
   "metadata": {},
   "source": [
    "## Implement other metrics\n",
    "\n",
    "\n",
    "### https://docs.confident-ai.com/docs/metrics-introduction#using-local-llm-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7746d0e-825a-4d40-9361-6776eaeff712",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ad1e2b-d8cf-478a-ad08-11490ab593b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
